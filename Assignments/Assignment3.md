# Assignment #3

## Question 1: Understanding Backpropagation in MLP  
Read the uploaded lecture notes **"MLP and Backpropagation.pdf"** and consider a Multi-Layer Perceptron (MLP) model with one input layer, one hidden layer, and one output layer, each consisting of **two** neurons.  

Using the same notations introduced in the notes, derive and simplify the following partial derivatives:  

\[
\frac{\partial L}{\partial w_{2,1}^{out}}, \quad  
\frac{\partial L}{\partial w_{2,2}^{out}}, \quad  
\frac{\partial L}{\partial b_1^{out}}, \quad  
\frac{\partial L}{\partial w_{1,2}^{h}}, \quad  
\frac{\partial L}{\partial b_2^{h}}  
\]

## Question 2: Experimenting with MLP Training  
Test the uploaded code **"demo_MLP_intro.ipynb"** and analyze its performance.  

- Test the accuracy for different learning rates.  
- Generate a contour plot of the accuracy in the plane of **"iteration (or epoch)"** and **"learning rate."**  

---

## ðŸ“Œ Notes  
- Let's **form new teams** to work on these problems.  
- Each team should submit **one** copy of their solutions for **Question 1** and their **code for Question 2** via email **no later than 2 PM, March 25th**.  
- At the beginning of the next lecture, all teams will briefly present their results and analysis.  
- The assessment will focus primarily on **effort and understanding**, rather than just the final results.  

- team A: Adam Mathieu, Xiang Wan
- team B: Esen Dashnyam, Christopher Cap
- team C: Kaifeng Liu, Harrison Teele
- team D: Matthew Daddino, Khalid Musa
- team E: Troy Chartier-Vignapiano, Son Nguyen

