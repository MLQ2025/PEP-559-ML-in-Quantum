{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d58289d-3035-4a74-af15-899eedbb25d0",
   "metadata": {},
   "source": [
    "# Implementing the 2D convolution using two approaches\n",
    "- manually\n",
    "- with the scipy.signal.convolve2d package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a9fa66-b4cb-4834-bbaf-d7efe1ded280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation:\n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "SciPy Results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    \"\"\"\n",
    "    2D convolution operation with padding and stride.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input 2D array\n",
    "    - W: Kernel or filter, also a 2D array\n",
    "    - p: Padding (pad_height, pad_width), i.e., add pad_height zeros to the top/bottom of the array, \n",
    "    - add pad_width zeros to the left/bottom of the array\n",
    "    - s: Stride (stride_height, stride_width)\n",
    "\n",
    "    Returns:\n",
    "    - Convolved 2D output\n",
    "    \"\"\"\n",
    "    # Rotate the kernel by 180 degrees\n",
    "    W_rot = np.array(W)[::-1, ::-1]\n",
    "\n",
    "    # Apply padding to the input array\n",
    "    X_orig = np.array(X)\n",
    "    pad_height, pad_width = p\n",
    "    padded_shape = (X_orig.shape[0] + 2 * pad_height, X_orig.shape[1] + 2 * pad_width)\n",
    "    X_padded = np.zeros(padded_shape)\n",
    "    X_padded[pad_height:pad_height + X_orig.shape[0],\n",
    "             pad_width:pad_width + X_orig.shape[1]] = X_orig\n",
    "    # The above line copies the original X_orig into the center of a larger padded array X_padded, leaving zeros around the border.\n",
    "\n",
    "    # Initialize output list\n",
    "    res = []\n",
    "\n",
    "    stride_y, stride_x = s\n",
    "    out_height = (X_padded.shape[0] - W_rot.shape[0]) // stride_y + 1\n",
    "    out_width = (X_padded.shape[1] - W_rot.shape[1]) // stride_x + 1\n",
    "\n",
    "    # Perform the convolution operation\n",
    "    for i in range(0, out_height * stride_y, stride_y):\n",
    "        row = []\n",
    "        for j in range(0, out_width * stride_x, stride_x):\n",
    "            X_sub = X_padded[i:i + W_rot.shape[0], j:j + W_rot.shape[1]]\n",
    "            row.append(np.sum(X_sub * W_rot))  # Element-wise multiply and sum\n",
    "        res.append(row)\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "# Test example\n",
    "X = [[1, 3, 2, 4],\n",
    "     [5, 6, 1, 3],\n",
    "     [1, 2, 0, 2],\n",
    "     [3, 4, 3, 2]]\n",
    "\n",
    "W = [[1, 0, 3],\n",
    "     [1, 2, 1],\n",
    "     [0, 1, 1]]\n",
    "\n",
    "print('Conv2d Implementation:\\n', conv2d(X, W, p=(1, 1), s=(1, 1)))\n",
    "\n",
    "# Compare with SciPy’s convolution (with 'same' mode to match padding)\n",
    "print('SciPy Results:\\n', scipy.signal.convolve2d(X, W, mode='same'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8577fb-e616-4275-988f-833767e50f60",
   "metadata": {},
   "source": [
    "# Implementing a CNN with PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-correlation\n",
    "\n",
    "- Most deep learning frameworks (including PyTorch) implement **cross-correlation** but refer to it as **convolution**.  \n",
    "- Cross-correlation is similar to traditional convolution, except that it does **not rotate the kernel** — it performs element-wise multiplication in the same orientation.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Representation\n",
    "\n",
    "- The input is typically a **3D array** of shape **(Height × Width × Channels)**.\n",
    "- For example:\n",
    "  - A **grayscale** image has 1 channel (C=1).\n",
    "  - An **RGB** image has 3 channels (C=3).\n",
    "- The convolution is applied **separately per channel**, and the filtered results are **summed element-wise** to form the output for each filter.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To improve generalization and prevent overfitting:\n",
    "- Regularization adds a **penalty term** to the loss function.\n",
    "- This discourages overly complex models and promotes simpler, more general patterns.\n",
    "- Regularization helps the model perform better on **unseen test data**.\n",
    "\n",
    "---\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Another key technique to prevent overfitting is **Dropout**:\n",
    "- During training, a fraction of the hidden units is **randomly dropped** in each iteration with a user-defined probability `p`.\n",
    "- A typical choice is `p = 0.5`.\n",
    "- Dropping forces the network to rely on **multiple paths** and discourages co-adaptation of neurons.\n",
    "- After dropping, the remaining units' outputs are **scaled** to preserve the overall magnitude of the activations.\n",
    "\n",
    "---\n",
    "\n",
    "The following cells implements a CNN for MNIST digit classification with **Dropout layers** for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26a753-8ff0-43e9-881f-73f466d64567",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4514fd-6d5f-43c4-bfa9-130b7708baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1d71f-4f68-4216-8477-00f051a9971b",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5a9f5a-2ff2-492b-83af-2b0bfee80f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Tensor Shape: torch.Size([60000, 28, 28])\n",
      "Data Type: torch.uint8\n",
      "Label Shape: torch.Size([60000])\n",
      "Unique Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the training data\n",
    "mnist_raw = datasets.MNIST(root='.', train=True, download=True)\n",
    "\n",
    "# Extract the image tensors and labels\n",
    "images = mnist_raw.data\n",
    "labels = mnist_raw.targets\n",
    "\n",
    "# show the shape and data type\n",
    "print(f\"Image Tensor Shape: {images.shape}\")\n",
    "print(f\"Data Type: {images.dtype}\") # unit8 stands for \"unsigned 8-bit integer\", so it can represent numbers from 0 to 255\n",
    "print(f\"Label Shape: {labels.shape}\")\n",
    "print(f\"Unique Labels: {labels.unique().tolist()}\")\n",
    "\n",
    "# print the first 6 images\n",
    "fig, axs = plt.subplots(1, 6, figsize=(12, 2))\n",
    "for i in range(6):\n",
    "    axs[i].imshow(images[i], cmap='gray')\n",
    "    axs[i].set_title(f\"Label: {labels[i]}\")\n",
    "    axs[i].axis('off')\n",
    "plt.suptitle(\"Sample MNIST Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbad63-5e9c-410e-9ec9-57abd4cd7207",
   "metadata": {},
   "source": [
    "## Analyze the mean/std of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc3ce7e-693a-4cf2-8714-e87dc2de6eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Mean: 0.1307\n",
      "MNIST Std: 0.3081\n"
     ]
    }
   ],
   "source": [
    "data = images.float() / 255.0 # convert to float and normalize to [0, 1]\n",
    "\n",
    "# compute the mean and std\n",
    "mnist_mean = data.mean().item() # .item() method in PyTorch is used to extract a single Python scalar from a PyTorch tensor\n",
    "mnist_std = data.std().item() # that contains only one element\n",
    "\n",
    "print(f\"MNIST Mean: {mnist_mean:.4f}\")\n",
    "print(f\"MNIST Std: {mnist_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dade834-79a0-49f6-b984-5bbce82ebc95",
   "metadata": {},
   "source": [
    "## Reload and transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c17ff37-7de1-49ca-ab7f-411c6f2d4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data so that it has zero mean and unit variance, which helps models train faster and more stably\n",
    "# We can transform the data automatically as it's being loaded\n",
    "\n",
    "# first, define a transform operation\n",
    "\n",
    "transform = transforms.Compose([    # combines together multiple transformation so that they can be applied in order to the data\n",
    "    transforms.ToTensor(),    # first converts the image from an numpy array or image array to a PyTorch tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # then normalizes the tensor using the mean and STD of the dataset\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
    "\n",
    "# prepare the dataset in mini-batches of size 64 and shuffle the data at each epoch\n",
    "# Each mini-batch will be fed into the model and the loss is comput4ed for each mini-batch. The optimizer updates the weights\n",
    "# after each mini-batch (standard SGD). At the end of the epoch, we typically aggregate the losses to evaluate the overall performance\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ad0ae-26a8-4920-a920-87a3fe47d6e1",
   "metadata": {},
   "source": [
    "## Defined the CNN Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19877d67-bd39-483b-9074-5013bfd09b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithDropout(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNWithDropout, self).__init__()\n",
    "\n",
    "        # First convolutional layer: input 1x28x28 --> output 10x24x24\n",
    "        # In PyTorch, the standard shape for image tensor is [Batch Size, Channels, Height, Width]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # Second convolutional layer: input 10x12x12 (why?) --> output 20x8x8\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # Dropout layer after conv2\n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(320, 50) # flattend size: 20x4x4 = 320\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(50, 10) # 10 output class (digts 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply conv --> ReLU --> MaxPool\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.dropout1(self.conv2(x)), 2))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1) # Log-Softmax for classification\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbfea6-5a63-44e7-b7c5-f8636cb53416",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db6456a-39c5-4edb-bc5a-6aa4d62fd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target) # Negative Log Likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa373b0-4fad-4d4e-a9b1-54387c4b659d",
   "metadata": {},
   "source": [
    "## Instantiate Model, Optimizer, and Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b0f542-ca53-4fe7-b0cb-433286041270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.2720\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.5630\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.5167\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.3291\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.2988\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.1830\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.2188\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.2599\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.3853\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.1947\n",
      "\n",
      "Test set: Avg. loss: 0.0813, Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "Train Epoch: 2 [0/60000] Loss: 0.3089\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.1187\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.1146\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.1931\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.3192\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.1335\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.1513\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.1236\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.1898\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.0651\n",
      "\n",
      "Test set: Avg. loss: 0.0499, Accuracy: 9840/10000 (98.40%)\n",
      "\n",
      "Train Epoch: 3 [0/60000] Loss: 0.0974\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.0657\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.0876\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.0753\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.1338\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.1597\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.0773\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.2206\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.1440\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.2193\n",
      "\n",
      "Test set: Avg. loss: 0.0432, Accuracy: 9858/10000 (98.58%)\n",
      "\n",
      "Train Epoch: 4 [0/60000] Loss: 0.0558\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.1003\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.0570\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.1382\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.1394\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.1508\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.1371\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.0242\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.1216\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.1109\n",
      "\n",
      "Test set: Avg. loss: 0.0395, Accuracy: 9874/10000 (98.74%)\n",
      "\n",
      "Train Epoch: 5 [0/60000] Loss: 0.1168\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.0557\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.0295\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.0642\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.0290\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.1083\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.0494\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.0507\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.0515\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.0930\n",
      "\n",
      "Test set: Avg. loss: 0.0374, Accuracy: 9889/10000 (98.89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNWithDropout().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 6): # Train for 5 epochs\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d21a3-412a-40a4-9c79-8bfe7d775ff3",
   "metadata": {},
   "source": [
    "## Visualize the architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65957a58-d53e-406c-a540-28b5e06f821e",
   "metadata": {},
   "source": [
    "### Use torchviz – For Graph Visualization\n",
    "\n",
    "- It draws the computation graph, great for understanding layer connections and flow.\n",
    "- generates a real graph (saved as PNG, PDF, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abea590b-787f-4ce6-845a-143f67944c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn_graph.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "import torch\n",
    "model = CNNWithDropout()\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "output = model(dummy_input)\n",
    "\n",
    "make_dot(output, params=dict(model.named_parameters())).render(\"cnn_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96f3ae-8127-4325-8ae1-06c2a3dc1a1b",
   "metadata": {},
   "source": [
    "### Use torchsummary to Quick and Easy Overview\n",
    "\n",
    "- Gives a compact, layer-by-layer summary\n",
    "- Input/output shapes, parameter counts, total memory usage\n",
    "- Use it when you want a quick idea of the model’s structure without visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d02f7c-cbf2-4775-81a0-cbaf9e4278ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "           Dropout-5                   [-1, 50]               0\n",
      "            Linear-6                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = CNNWithDropout()\n",
    "summary(model, input_size=(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be6083-23b6-4b83-a5bb-6e728cfa8595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
